#!/usr/bin/env node

const fs = require('fs');
const { spawn } = require('child_process');

const proj_root = function(...path){ return(__dirname + "/../" + path.join("/")); } 

const config = require(proj_root("config.js"));

const _ = require("../lib/_.js");
const openai = require("../lib/openai.js")(config.openai);

const _context_warning_limit = -1;
// const _context_warning_limit = 8192;

const _models = require("../lib/models.js");
const _helpers = require("../lib/helpers.js");
const { warn } = require('console');

// TODO: use dry.baseline _.os.copy once it's implemented;

function pbcopy(data) {
   var proc = require('child_process').spawn('pbcopy'); 
   proc.stdin.write(data); proc.stdin.end();
}

const _colors = {
   "system": 178,
   "user": 166,
   "model": 46,
   "warning": 196
}

function warn_about_context_window({ model, token_count }){
   if(model.context_window < _context_warning_limit){ return; }

   const model_total = model.context_window;

   const model_limit = (model_total / 4);

   if(token_count > model_limit){
      const warning = `warning: input token count (${token_count}) exceeds model warning limit (${model_limit}) of total model context length (${model_total}). the model may lose track of facts in the middle of the window.`;
      _.line("");
      _.line(_.color_256(_colors.warning, warning))
      _.line("");
   }
}

function dollars(n){
   const cents = n * 100;
   if(cents > 100){
      return ("$" + n.toFixed(2));
   }else{
      return (cents.toFixed(2) + "Â¢");
   }
}

async function do_llm({ model, messages }){
   const start = Date.now();

   _.line("");

   const n_gutter = 8; // system: == 8 chars
   const n_wrap = 80; 

   const chat = messages.map(function(v){
      const label = (v.role + ":");

      const wrapper = _.text_wrapper({
         n_wrap,
         n_gutter,
         // n_gutter: (label.length + 1),
         n_leading: label.length,
      });

      const content = wrapper.once({ chunk: v.content });

      return(_.color_256(_colors[v.role], label) + content);

   }).join("\n\n");

   const input_token_count = model.count_tokens_from_messages(messages);

   warn_about_context_window({ model, token_count: input_token_count });

   _.out(chat);

   _.line("");
   _.line("");

   const model_label = (model.short_name + ":");

   _.out(_.color_256(_colors["model"],  model_label));

   const wrapper = _.text_wrapper({
      n_wrap,
      n_gutter,
      // n_gutter: (model_label.length + 1) 
      n_leading: model_label.length,
   });

   const { content: output } = await openai.stream({ model, messages, out_f: function(chunk){
      _.out(chunk + "|");
      // _.out(wrapper.stream({ chunk }));
   }});

   pbcopy(output);

   const price_in = model.price.messages_in(messages);
   const price_out = model.price.text_out(output);

   _.line("");
   _.line("");

   const table = {
      "took": ((Date.now() - start) + "ms"),
      "tokens (input)": _.to_k(price_in.count),
      "tokens (output)": _.to_k(price_out.count),
      "price (input)": dollars(price_in.price),
      "price (output)": dollars(price_out.price),
      "price (total)": dollars(price_in.price + price_out.price),
      "model (version)": model.short_name,
   };

   let max_label = 0;
   let max_value = 0;

   _.each.s(table, function(v, k){
      max_label = Math.max(k.length, max_label);
      max_value = Math.max((v+"").length, max_value);
   });

   _.each.s(table, function(v, k){ 
      _.line(k.padEnd(max_label) + " : ", (v + "").padStart(max_value));
   });

   _.line("");
   _.line("(response was copied to the cliboard)");

}

function help(no_exit){

   _.line("usage: llm <model | alias | cont> [<helper_name> [prompt]]");
   _.line("aliases: ", pretty_obj(_models.help().aliases));
   _.line("models: ", pretty_obj(_models.help().models));
   _.line("helpers: ", to_help_keys(_helpers));

   if(no_exit){ return; }

   process.exit(1); 
}

async function handle_options(argv){

   let helper = _helpers["default"];
   let model_name = argv.shift();
   let prompt = "";

   if(!model_name || model_name === "help" || model_name === "--help"){ return help(); }

   if(model_name === "cont"){ _.fatal("cont not implemented"); }

   const model = _models.get(model_name);

   if(!model){
      _.line("unknown model type: ", model_name, " you must pick a model or alias in the list below.");
      return help();
   }
   
   if(argv.length){
      const helper_name = argv.shift();
      helper = _helpers[helper_name];
      prompt = argv.join(" ").trim();
      if(!helper){ 
         _.line(`unknown helper "`, helper_name, `": need one in the list below.`);
         return help();
      }
   }

   if(!prompt){ prompt = await get_prompt(_helpers.to_editor_text(helper)); }

   let messages = null;

   if(prompt){ messages = helper(prompt); }

   return({ model, messages });
}

async function get_prompt(initial_text){

   function open_editor(path){
      return new Promise((resolve, reject) => {
         const editor = process.env.EDITOR || 'vim';

         const args = [];

         if(editor === "vim" || editor === "nvim"){
            args.push("-c"); args.push("$"); args.push("+start");
         }

         args.push(path);

         const child = spawn(editor, args, { stdio: 'inherit' });

         child.on('exit', function(code){ 
            if(code){ reject(_.error("editor_exit", "editor exited with code: ", code)); }
            else{ resolve(); }
         });
         child.on('error', function(err){ reject(err); });
      });
   }

   const prompt_path = "/tmp/llm_prompt_" + _.timestamp();

   initial_text = [
      `// please enter your prompt below. lines starting with "//" are ignored and an empty message aborts the process.`,
      initial_text,
      "", ""
   ].join("\n");

   fs.writeFileSync(prompt_path, initial_text);

   try{
      await open_editor(prompt_path);
   }catch(e){
      if(e.code === "editor_exit"){ return(""); }
   }

   const prompt = fs.readFileSync(prompt_path, "utf-8");
   const lines = prompt.split("\n").filter(function(line){ return !line.startsWith('//'); });

   fs.unlinkSync(prompt_path);

   return(lines.join("\n").trim());
}

function pretty_obj(obj){ return JSON.stringify(obj, null, 3); }
function to_help_keys(obj){ return(pretty_obj(Object.keys(obj))); }

function round_trip_test({ model, text }){
   
   const tokens = model.to_tokens(text);
   const decode = model.from_tokens(tokens);

   _.line(`tokens.length : `, tokens.length);
   _.line(`tokens.encoded: "`, JSON.stringify(tokens), `"`);
   _.line(`tokens.decoded: "`, decode, `"`);
   _.line(`tokens.source : "`, text, `"`);
}

function debug({ model, messages }){

   _.line("model: ", model);
   _.line("messages: ", messages);

   const prompt = messages[messages.length-1].content;

   _.line("price.in: ", model.price.messages_in(messages));

   const str_out = "hello! how can I help you?"

   _.line(`text.out: "`, str_out, `"`);
   _.line("price.out: ", model.price.text_out(str_out));
}

// llm_cli | tee >(pbcopy)
(async function(){

   // return round_trip_test({ model: _models.get("gpt-4"), text: "this is a test message." });

   const argv = process.argv.slice(2);

   const { model, messages } = await handle_options(argv);

   if(!messages){ return _.line("empty prompt. canceling."); }

   // return debug({ model, messages });

   await do_llm({ model, messages });

})();

